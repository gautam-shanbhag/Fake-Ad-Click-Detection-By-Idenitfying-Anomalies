# -*- coding: utf-8 -*-
"""Fake Ad Click Detection

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cvAndzGDTV10DvaY032q6Wbwr92-lDpW

#Inital Run
"""

# Run this cell to mount your Google Drive.
from google.colab import drive
drive.mount('/content/drive')

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

import os
import gc
import matplotlib.pyplot as plt
import seaborn as sns
import datetime
import time
# %matplotlib inline

pal = sns.color_palette()
# Input data files are available in the "../input/" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory

import os
# Any results you write to the current directory are saved as output.

#make wider graphs
sns.set(rc={'figure.figsize':(12,5)});
plt.figure(figsize=(12,5));

#data = pd.read_csv('/content/drive/My Drive/Practicum Data Set/final_dataset.csv', index_col=0)
fullData = pd.read_csv('/content/drive/My Drive/Practicum Data Set/fullData.csv', index_col=0)
underSample = pd.read_csv('/content/drive/My Drive/Practicum Data Set/underSample.csv', index_col=0)
merge = pd.read_csv('/content/drive/My Drive/Practicum Data Set/merge.csv', index_col=0)

"""## Initial Data Desc"""

pip install impyute

pip install fancyimpute

pub1 = pd.read_csv('/content/drive/My Drive/Practicum Data Set/publishers_08mar12.csv')
pub2 = pd.read_csv('/content/drive/My Drive/Practicum Data Set/publishers_09feb12.csv')
pub3= pd.read_csv('/content/drive/My Drive/Practicum Data Set/publishers_23feb12.csv')

frames = [pub1,pub2,pub3]

pub = pd.concat(frames,sort=True)

click1 = pd.read_csv('/content/drive/My Drive/Practicum Data Set/clicks_08mar12.csv')
click2 = pd.read_csv('/content/drive/My Drive/Practicum Data Set/clicks_09feb12.csv')
click3 = pd.read_csv('/content/drive/My Drive/Practicum Data Set/clicks_23feb12.csv')

frames2 =[click1,click2,click3]

click = pd.concat(frames2,sort=True)

#pub1 = pd.read_csv('C:/Users/Gautam/Desktop/DCU/Practicum/Dataset/BuzzFeed Data/publishers_08mar12.csv')
#pub2 = pd.read_csv('C:/Users/Gautam/Desktop/DCU/Practicum/Dataset/BuzzFeed Data/publishers_09feb12.csv')
#pub3= pd.read_csv('C:/Users/Gautam/Desktop/DCU/Practicum/Dataset/BuzzFeed Data/publishers_23feb12.csv')

#frames = [pub1,pub2,pub3]

#pub = pd.concat(frames,sort=True)

#click1 = pd.read_csv('C:/Users/Gautam/Desktop/DCU/Practicum/Dataset/BuzzFeed Data/clicks_08mar12.csv')
#click2 = pd.read_csv('C:/Users/Gautam/Desktop/DCU/Practicum/Dataset/BuzzFeed Data/clicks_09feb12.csv')
#click3 = pd.read_csv('C:/Users/Gautam/Desktop/DCU/Practicum/Dataset/BuzzFeed Data/clicks_23feb12.csv')

#frames2 =[click1,click2,click3]

#click = pd.concat(frames2,sort=True)

data_test1 = pd.merge(click1, pub1, on='partnerid')
data_test2 = pd.merge(click2, pub2, on='partnerid')
data_test3 = pd.merge(click3, pub3, on='partnerid')

data_test1['status'] = data_test1['status'].replace('Observation_New','Observation')
data_test1 = data_test1[data_test1.status != 'Observation']
data_test1 = data_test1.drop(columns=['id','partnerid','cid','address','bankaccount'])

data_test2['status'] = data_test2['status'].replace('Observation_New','Observation')
data_test2 = data_test2[data_test2.status != 'Observation']
data_test2 = data_test2.drop(columns=['id','partnerid','cid','address','bankaccount'])

data_test3['status'] = data_test3['status'].replace('Observation_New','Observation')
data_test3 = data_test3[data_test3.status != 'Observation']
data_test3 = data_test3.drop(columns=['id','partnerid','cid','address','bankaccount'])

print(data_test1.groupby('status').count())
print(data_test1['status'].size)
print (data_test1.timeat.min())
print (data_test1.timeat.max())


print(data_test2.groupby('status').count())
print(data_test2['status'].size)
print (data_test2.timeat.min())
print (data_test2.timeat.max())


print(data_test3.groupby('status').count())
print(data_test3['status'].size)
print (data_test3.timeat.min())
print (data_test3.timeat.max())

pub.head(5)

click.head(5)

data = pd.merge(click, pub, on='partnerid')

#data.to_csv('dataset.csv',index=False)

"""COLUMN Description :

partnerid - partnerid - Unique identifier of a publisher.

Bankaccount - Bank account associated with a publisher (may be empty)

address - Mailing address of a publisher (obfuscated; may be empty)

status - Label of a publisher, which can be the following:
"OK" - Publishers whom BuzzCity deems as having healthy traffic (or those who slipped their detection mechanisms)
"Observation" - Publishers who may have just started their traffic or their traffic statistics deviates from system wide average. BuzzCity does not have any conclusive stand with these publishers yet
"Fraud" - Publishers who are deemed as fraudulent with clear proof. Buzzcity suspends their accounts and their earnings will not be paid


agent - deviceua - Phone model used by a clicker/visitor

category - publisherchannel - Publisher's channel type, which can be the following:
ad - Adult sites
co - Community
es - Entertainment and lifestyle
gd - Glamour and dating
in - Information
mc - Mobile content
pp - Premium portal
se - Search, portal, services
mg - 
ow - 

cid - adscampaignid - Unique identifier of a given advertisement campaign

cntr - usercountry - Country from which the surfer is

id - id - Unique identifier of a particular click

iplong - numericip - Public IP address of a clicker/visitor

referer - referredurl - URL where the ad banners were clicked (obfuscated; may be empty). 

timeat - clicktime - Timestamp of a given click (in YYYY-MM-DD format)

partnerid - publisherid - Unique identifier of a publisher
"""

data.head(5)

data['status'] = data['status'].replace('Observation_New','Observation')

data = data[data.status != 'Observation']

data.head(5)

data = data.drop(columns=['id','partnerid','cid','address','bankaccount'])

!pwd

#data.to_csv(path_or_buf ='/content/drive/My Drive/Practicum Data Set/final_dataset.csv',sep=',', header=True)



"""## Visualisations"""

plt.figure(figsize=(15, 8))
cols = data.columns
uniques = [len(fullData[col].unique()) for col in cols]
sns.set(font_scale=1.2)
ax = sns.barplot(cols, uniques, palette=pal, log=True)
ax.set(xlabel='Feature', ylabel='log(unique count)', title='Number of unique values per feature')
for p, uniq in zip(ax.patches, uniques):
    height = p.get_height()
    ax.text(p.get_x()+p.get_width()/2.,
            height + 10,
            uniq,
            ha="center")

print(data['category'].value_counts())

print(data['cntr'].value_counts())

print(data['agent'].value_counts())

# %matplotlib inline
import seaborn as sns
import matplotlib.pyplot as plt
carrier_count = data['category'].value_counts()
sns.set(style="darkgrid")
sns.barplot(carrier_count.index, carrier_count.values, alpha=0.9)
plt.title('Frequency Distribution By Category')
plt.ylabel('Number of Occurrences', fontsize=12)
plt.xlabel('Category', fontsize=12)
plt.show()

labels = data['category'].astype('category').cat.categories.tolist()
counts = data['category'].value_counts()
sizes = [counts[var_cat] for var_cat in labels]
fig1, ax1 = plt.subplots()
ax1.pie(sizes, labels=labels, autopct='%1.1f%%', shadow=True) #autopct is show the % on plot
ax1.axis('equal')
plt.show()

data.count()

data.isnull().sum()

fullData = data.dropna()

fullData.columns

cols = fullData.columns
uniques = [len(fullData[col].unique()) for col in cols]
sns.set(font_scale=1.2)
ax = sns.barplot(cols, uniques, palette=pal, log=True)
ax.set(xlabel='Feature', ylabel='log(unique count)', title='Number of unique values per feature')
for p, uniq in zip(ax.patches, uniques):
    height = p.get_height()
    ax.text(p.get_x()+p.get_width()/2.,
            height + 10,
            uniq,
            ha="center")

fullData.count()

print(fullData['status'].value_counts())

fullData.isnull().sum()

fullData['iplong'] = fullData['iplong'].astype('category')

#fullData.to_csv(path_or_buf ='/content/drive/My Drive/Practicum Data Set/fullData.csv',sep=',', header=True)

fullData.dtypes

no_frauds = len(fullData[fullData['status'] == 'Fraud'])
no_frauds
non_fraud_indices = fullData[fullData.status == 'OK'].index
random_indices = np.random.choice(non_fraud_indices,no_frauds, replace=False)
fraud_indices = fullData[fullData.status == 'Fraud'].index
under_sample_indices = np.concatenate([fraud_indices,random_indices])
underSample = fullData.loc[under_sample_indices]
#underSample.to_csv(path_or_buf ='/content/drive/My Drive/Practicum Data Set/underSample.csv',sep=',', header=True)

print(underSample['status'].value_counts())

underSample.columns

underSample['iplong'] = underSample['iplong'].astype('category')

underSample.dtypes

"""# Categorical Base Model"""

pip install catboost



"""First Base Model - CATBOOST"""

from catboost import CatBoostClassifier
from sklearn.model_selection import train_test_split
x = fullData.drop(columns=['status','referer'])
y = fullData['status'].copy()
y_mod = y.apply(lambda x: 0 if x == 'OK' else 1)

x_train, x_test, y_train, y_test = train_test_split(x, y_mod, random_state=13)

x.columns

y = y.to_frame()

y.columns

model_dataset = pd.concat([x_train, y_train], axis=1)

print(model_dataset['status'].value_counts())

no_frauds = len(model_dataset[model_dataset['status'] == 1])
non_fraud_indices = model_dataset[model_dataset.status == 0].index
random_indices = np.random.choice(non_fraud_indices,no_frauds, replace=False)
fraud_indices = model_dataset[model_dataset.status == 1].index
under_sample_indices = np.concatenate([fraud_indices,random_indices])
underSample = model_dataset.loc[under_sample_indices]
#underSample.to_csv(path_or_buf ='/content/drive/My Drive/Practicum Data Set/underSample.csv',sep=',', header=True)

print(underSample['status'].value_counts())

y_train = underSample.status
x_train = underSample.drop(['status'], axis=1)

model=CatBoostClassifier(random_state=13)
model.fit(x_train, y_train, cat_features=x_train.columns)

imp_features=model.feature_importances_
print(x.columns)
print(imp_features)

from sklearn import metrics 
y_pred=model.predict(x_test)

print("Accuracy:",metrics.accuracy_score(y_test, y_pred))

from sklearn.metrics import roc_auc_score

# Calculate roc auc
roc_value = roc_auc_score(y_test, y_pred)
roc_value

from sklearn.metrics import confusion_matrix, classification_report
print(classification_report(y_test, y_pred))
confusion_matrix(y_test, y_pred)

feature_imp = pd.Series(model.feature_importances_,index=x.columns).sort_values(ascending=False)
feature_imp

import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline
# Creating a bar plot
sns.barplot(x=feature_imp, y=feature_imp.index)
# Add labels to your graph
plt.xlabel('Feature Importance Score')
plt.ylabel('Features')
plt.title("Visualizing Important Features")
plt.legend()
plt.show()

x.dtypes

"""## TP"""

from imblearn.over_sampling import SMOTENC
sm = SMOTENC(categorical_features=x.columns,random_state=0)
x_train_res, y_train_res = sm.fit_sample(x_train, y_train)

cols_to_check = ['agent', 'cntr','referer']
data['is_na'] = data[cols_to_check].isnull().apply(lambda x: all(x), axis=1) 
data.head()  
print(data['is_na'].value_counts())

print(data.info())

cols = data.copy()

"""Handle Missing Values for agent, country, referer 

Not using address & bank account thus ignored
"""

cols = cols.fillna(method = 'bfill')

cols.isnull().sum()



temp = KNN(k=5).complete(cols)

cols = cols.drop(["status"])

from fancyimpute import  KNN
train_columns = list(cols)
train = KNN(cols,3)
train.columns = train_columns
#train.material = train.material.astype("object")
train = pd.get_dummies(train)

train

#We use the train dataframe from Titanic dataset
#fancy impute removes column names.
train_cols = list(cols)
# Use 5 nearest rows which have a feature to fill in each row's
# missing features
train = KNN(cols,5)
train.columns = train_cols





"""# FEATURE EXTRACTION:

## Feature Extraction
"""

#make wider graphs
sns.set(rc={'figure.figsize':(12,5)});
plt.figure(figsize=(12,5));

fullData.head(5)

fullData.columns

fullData.dtypes

fullData['status_binary'] = fullData['status'] .apply(lambda x: 0 if x == 'OK' else 1)

fullData['datetime'] = pd.to_datetime(fullData['timeat'])

fullData['click_month']=fullData['datetime'].dt.month
fullData['click_day']=fullData['datetime'].dt.day
fullData['click_hour']=fullData['datetime'].dt.hour
fullData['click_min']=fullData['datetime'].dt.minute
fullData['click_sec']=fullData['datetime'].dt.second

fullData.groupby('datetime').agg({'status':'count'}).plot(figsize=(12,6))
plt.ylabel('Number of clicks')
plt.title('Number of clicks by hour');

fullData[['click_hour','status_binary']].groupby(['click_hour'], as_index=True).count().plot(kind='bar', color='#a675a1')
plt.title('HOURLY CLICK FREQUENCY Barplot');
plt.ylabel('Number of Clicks');

fullData[['click_hour','status_binary']].groupby(['click_hour'], as_index=True).count().plot(color='#a675a1')
plt.title('HOURLY CLICK FREQUENCY Lineplot');
plt.ylabel('Number of Clicks');

fraudData = fullData[fullData['status_binary'] == 1]

fraudData[['click_hour','status_binary']].groupby(['click_hour'], as_index=True).count().plot( color='#75a1a6')
plt.title('HOURLY CONVERSION RATIO Lineplot');
plt.ylabel('Converted Ratio');

fullData[['click_hour','status_binary']].groupby(['click_hour'], as_index=True).mean().plot(kind='bar', color='#75a1a6')
plt.title('HOURLY CONVERSION RATIO Barplot');
plt.ylabel('Converted Ratio');

fullData[['click_hour','status_binary']].groupby(['click_hour'], as_index=True).mean().plot( color='#75a1a6')
plt.title('HOURLY CONVERSION RATIO Lineplot');
plt.ylabel('Converted Ratio');

group = fullData[['click_hour','status_binary']].groupby(['click_hour'], as_index=False).mean()
x = group['click_hour']
ymean = group['status_binary']
group = fraudData[['click_hour','status_binary']].groupby(['click_hour'], as_index=False).count()
ycount = group['status_binary']


fig = plt.figure()
host = fig.add_subplot(111)

par1 = host.twinx()

host.set_xlabel("Time")
host.set_ylabel("Click Count")
par1.set_ylabel("Fraud Count")

#host.set_ylim([0,  600000])
#par1.set_ylim([0,  60000])

#color1 = plt.cm.viridis(0)
#color2 = plt.cm.viridis(0.5)
color1 = '#75a1a6'
color2 = '#a675a1'

p1, = host.plot(x, ymean, color=color1,label="Click Count")
p2, = par1.plot(x, ycount, color=color2, label="Fraud Count")

lns = [p1, p2]
host.legend(handles=lns, loc='best')

host.yaxis.label.set_color(p1.get_color())
par1.yaxis.label.set_color(p2.get_color())

plt.savefig("pyplot_multiple_y-axis.png", bbox_inches='tight')

group = fullData[['click_hour','status_binary']].groupby(['click_hour'], as_index=False).mean()
x = group['click_hour']
ymean = group['status_binary']
group = fullData[['click_hour','status_binary']].groupby(['click_hour'], as_index=False).count()
ycount = group['status_binary']


fig = plt.figure()
host = fig.add_subplot(111)

par1 = host.twinx()

host.set_xlabel("Time (per Hour)")
host.set_ylabel("False Proportion")
par1.set_ylabel("Total Click Count")

#color1 = plt.cm.viridis(0)
#color2 = plt.cm.viridis(0.5)
color1 = '#75a1a6'
color2 = '#a675a1'

p1, = host.plot(x, ymean, color=color1,label="False Proportion")
p2, = par1.plot(x, ycount, color=color2, label="Total Click Count")

lns = [p1, p2]
host.legend(handles=lns, loc='best')

host.yaxis.label.set_color(p1.get_color())
par1.yaxis.label.set_color(p2.get_color())

plt.savefig("pyplot_multiple_y-axis.png", bbox_inches='tight')

Index(['agent', 'category', 'cntr', 'iplong', 'referer', 'timeat', 'status',
       'status_binary', 'datetime', 'click_day', 'click_hour', 'click_min',
       'click_sec'],
      dtype='object')

merge.columns

"""## COUNT FEATURES"""

# Count the number of clicks by ip
ip_count = fullData.groupby(['iplong'])['timeat'].count().reset_index()
ip_count.columns = ['iplong', 'clicks_by_ip']
merge= pd.merge(fullData, ip_count, on='iplong', how='left', sort=False)
merge['clicks_by_ip'] = merge['clicks_by_ip'].astype('uint16')
#merge.drop('ip', axis=1, inplace=True)

# Count the number of clicks by country
cntr_count = fullData.groupby(['cntr'])['timeat'].count().reset_index()
cntr_count.columns = ['cntr', 'clicks_by_cntr']
merge = pd.merge(merge, cntr_count, on='cntr', how='left', sort=False)
merge['clicks_by_cntr'] = merge['clicks_by_cntr'].astype('uint16')
#merge.drop('ip', axis=1, inplace=True)

# Count the number of clicks by referer
referer_count = fullData.groupby(['referer'])['timeat'].count().reset_index()
referer_count.columns = ['referer', 'clicks_by_referer']
merge = pd.merge(merge, referer_count, on='referer', how='left', sort=False)
merge['clicks_by_referer'] = merge['clicks_by_referer'].astype('uint16')
#merge.drop('ip', axis=1, inplace=True)

# Count the number of clicks by category
category_count = fullData.groupby(['category'])['timeat'].count().reset_index()
category_count.columns = ['category', 'clicks_by_category']
merge = pd.merge(merge, category_count, on='category', how='left', sort=False)
merge['clicks_by_category'] = merge['clicks_by_category'].astype('uint16')
#merge.drop('ip', axis=1, inplace=True)

# Count the number of clicks per minute
minute_count = fullData.groupby(['click_month','click_day','click_hour','click_min'])['timeat'].count().reset_index()
minute_count.columns = ['click_month','click_day','click_hour','click_min', 'clicks_per_minute']
merge = pd.merge(merge, minute_count, on=['click_month','click_day','click_hour','click_min'], how='left', sort=False)
merge['clicks_per_minute'] = merge['clicks_per_minute'].astype('uint16')
#merge.drop('ip', axis=1, inplace=True)

# Count the number of clicks per hour
hour_count = fullData.groupby(['click_month','click_day','click_hour'])['timeat'].count().reset_index()
hour_count.columns = ['click_month','click_day','click_hour', 'clicks_per_hour']
merge = pd.merge(merge, hour_count, on=['click_month','click_day','click_hour'], how='left', sort=False)
merge['clicks_per_hour'] = merge['clicks_per_hour'].astype('uint16')
#merge.drop('ip', axis=1, inplace=True)

# Count the number of clicks per ip, referrer, category per min
ip_ref_cat_min_count = fullData.groupby(['iplong','referer','category','click_month','click_day','click_hour','click_min'])['timeat'].count().reset_index()
ip_ref_cat_min_count.columns = ['iplong','referer','category','click_month','click_day','click_hour','click_min', 'ip_ref_cat_per_min']
merge = pd.merge(merge, ip_ref_cat_min_count, on=['iplong','referer','category','click_month','click_day','click_hour','click_min'], how='left', sort=False)
merge['ip_ref_cat_per_min'] = merge['ip_ref_cat_per_min'].astype('uint16')
#merge.drop('ip', axis=1, inplace=True)

# Count the number of clicks per ip, referrer, category per hr
ip_ref_cat_hr_count = fullData.groupby(['iplong','referer','category','click_month','click_day','click_hour'])['timeat'].count().reset_index()
ip_ref_cat_hr_count.columns = ['iplong','referer','category','click_month','click_day','click_hour', 'ip_ref_cat_per_hr']
merge = pd.merge(merge, ip_ref_cat_hr_count, on=['iplong','referer','category','click_month','click_day','click_hour'], how='left', sort=False)
merge['ip_ref_cat_per_hr'] = merge['ip_ref_cat_per_hr'].astype('uint16')
#merge.drop('ip', axis=1, inplace=True)

# Count the number of clicks per ip, category per min
ip_cat_min_count = fullData.groupby(['iplong','category','click_month','click_day','click_hour','click_min'])['timeat'].count().reset_index()
ip_cat_min_count.columns = ['iplong','category','click_month','click_day','click_hour','click_min', 'ip_cat_per_min']
merge = pd.merge(merge, ip_cat_min_count, on=['iplong','category','click_month','click_day','click_hour','click_min'], how='left', sort=False)
merge['ip_cat_per_min'] = merge['ip_cat_per_min'].astype('uint16')
#merge.drop('ip', axis=1, inplace=True)

# Count the number of clicks per ip, category per hr
ip_cat_hr_count = fullData.groupby(['iplong','category','click_month','click_day','click_hour'])['timeat'].count().reset_index()
ip_cat_hr_count.columns = ['iplong','category','click_month','click_day','click_hour', 'ip_cat_per_hr']
merge = pd.merge(merge, ip_cat_hr_count, on=['iplong','category','click_month','click_day','click_hour'], how='left', sort=False)
merge['ip_cat_per_hr'] = merge['ip_cat_per_hr'].astype('uint16')
#merge.drop('ip', axis=1, inplace=True)

# Count the number of clicks by agent
agent_count = fullData.groupby(['agent'])['timeat'].count().reset_index()
agent_count.columns = ['agent', 'clicks_by_agent']
merge= pd.merge(merge, agent_count, on='agent', how='left', sort=False)
merge['clicks_by_agent'] = merge['clicks_by_agent'].astype('uint16')
#merge.drop('ip', axis=1, inplace=True)

# Count the number of clicks by agent,ip
agent_ip_count = fullData.groupby(['agent','iplong'])['timeat'].count().reset_index()
agent_ip_count.columns = ['agent','iplong','agent_ip_count']
merge= pd.merge(merge, agent_ip_count, on=['agent','iplong'], how='left', sort=False)
merge['agent_ip_count'] = merge['agent_ip_count'].astype('uint16')
#merge.drop('ip', axis=1, inplace=True)

# Count the number of clicks by agent,cntr
agent_cntr_count = fullData.groupby(['agent','cntr'])['timeat'].count().reset_index()
agent_cntr_count.columns = ['agent','cntr','agent_cntr_count']
merge= pd.merge(merge, agent_cntr_count, on=['agent','cntr'], how='left', sort=False)
merge['agent_cntr_count'] = merge['agent_cntr_count'].astype('uint16')
#merge.drop('ip', axis=1, inplace=True)

"""AVERAGE FEATURES"""

# Average number of clicks per ip per min
ip_min_count = merge.groupby(['iplong','click_month','click_day','click_hour','click_min'])['timeat'].count().reset_index()
ip_min_count.columns = ['iplong','click_month','click_day','click_hour','click_min', 'ip_min_count']
merge = pd.merge(merge, ip_min_count, on=['iplong','click_month','click_day','click_hour','click_min'], how='left', sort=False)
merge['ip_min_count'] = merge['ip_min_count'].astype('uint16')
#merge.drop('ip', axis=1, inplace=True)

# Average number of clicks per ip per min
ip_min_avg = merge.groupby(['iplong'])['ip_min_count'].mean().reset_index()
ip_min_avg.columns = ['iplong', 'ip_min_avg']
merge = pd.merge(merge, ip_min_avg, on=['iplong'], how='left', sort=False)
merge['ip_min_avg'] = merge['ip_min_avg'].astype('uint16')
#merge.drop('ip', axis=1, inplace=True)

# Average number of clicks per ip, referrer, category per min
ip_ref_cat_min_avg = merge.groupby(['iplong','referer','category','click_month','click_day','click_hour','click_min'])['ip_ref_cat_per_min'].mean().reset_index()
ip_ref_cat_min_avg.columns = ['iplong','referer','category','click_month','click_day','click_hour','click_min', 'ip_ref_cat_min_avg']
merge = pd.merge(merge, ip_ref_cat_min_avg, on=['iplong','referer','category','click_month','click_day','click_hour','click_min'], how='left', sort=False)
merge['ip_ref_cat_min_avg'] = merge['ip_ref_cat_min_avg'].astype('uint16')
#merge.drop('ip', axis=1, inplace=True)

# Average number of clicks per ip, category per min
ip_cat_min_avg = merge.groupby(['iplong','category','click_month','click_day','click_hour','click_min'])['ip_cat_per_min'].mean().reset_index()
ip_cat_min_avg.columns = ['iplong','category','click_month','click_day','click_hour','click_min', 'ip_cat_min_avg']
merge = pd.merge(merge, ip_cat_min_avg, on=['iplong','category','click_month','click_day','click_hour','click_min'], how='left', sort=False)
merge['ip_cat_min_avg'] = merge['ip_cat_min_avg'].astype('uint16')
#merge.drop('ip', axis=1, inplace=True)

merge.head(5)

merge.columns

merge = merge.drop(["ip_min_count_x","ip_min_count_y"],axis=1)

merge.to_csv(path_or_buf ='/content/drive/My Drive/Practicum Data Set/merge.csv',sep=',', header=True)

"""## Modelling"""

model_dataset = merge.copy()

print(model_dataset['status'].value_counts())

model_dataset.columns



"""# MODEL 1:

## Under Sampling &Final Test & Train Split
"""

from sklearn.model_selection import train_test_split
x = model_dataset.drop(columns=['agent','category', 'cntr', 'iplong', 'referer','timeat','status','status_binary','datetime','click_day', 'click_hour', 'click_min',
       'click_sec', 'click_month'])
y = model_dataset['status'].copy()
y_mod = y.apply(lambda x: 0 if x == 'OK' else 1)

x_train, x_test, y_train, y_test = train_test_split(x, y_mod, random_state=13)

model_dataset = pd.concat([x_train, y_train], axis=1)

model_dataset.head()

no_frauds = len(model_dataset[model_dataset['status'] == 1])
non_fraud_indices = model_dataset[model_dataset.status == 0].index
random_indices = np.random.choice(non_fraud_indices,no_frauds, replace=False)
fraud_indices = model_dataset[model_dataset.status == 1].index
under_sample_indices = np.concatenate([fraud_indices,random_indices])
underSample = model_dataset.loc[under_sample_indices]
#underSample.to_csv(path_or_buf ='/content/drive/My Drive/Practicum Data Set/underSample.csv',sep=',', header=True)

y_train = underSample.status
x_train = underSample.drop('status', axis=1)

"""## One Hot Encoding"""

x = pd.get_dummies(x, columns = ['agent'],prefix=['agent_'])

x.columns

x.shape

"""## Train Test Split"""

#x_train, x_test, y_train, y_test = train_test_split(x, y_mod, random_state=13)

#x_train.head(5)

cols = ['clicks_by_ip', 'clicks_by_cntr',
       'clicks_by_referer', 'clicks_by_category', 'clicks_per_minute',
       'clicks_per_hour', 'ip_ref_cat_per_min', 'ip_ref_cat_per_hr',
       'ip_cat_per_min', 'ip_cat_per_hr', 'ip_min_count', 'ip_min_avg',
       'ip_ref_cat_min_avg', 'ip_cat_min_avg', 'clicks_by_agent',
       'agent_ip_count', 'agent_cntr_count']

x_train_temp = x_train

x_test_temp = x_test

corr = x_train.corr()
corr.style.background_gradient(cmap='coolwarm')
corr.style.background_gradient(cmap='coolwarm').set_precision(2)

x_train = x_train_temp[['clicks_by_ip', 'clicks_by_cntr',
       'clicks_by_referer', 'clicks_by_category', 'clicks_per_minute',
       'clicks_per_hour',  'ip_cat_min_avg', 'clicks_by_agent',
       'agent_ip_count', 'agent_cntr_count']]

x_test = x_test_temp[['clicks_by_ip', 'clicks_by_cntr',
       'clicks_by_referer', 'clicks_by_category', 'clicks_per_minute',
       'clicks_per_hour', 'ip_cat_min_avg', 'clicks_by_agent',
       'agent_ip_count', 'agent_cntr_count']]

#from sklearn import preprocessing
#mm_scaler = preprocessing.Normalizer(copy=False)
#x_train = mm_scaler.fit_transform(x_train)
#x_test = mm_scaler.transform(x_test)

from sklearn import preprocessing
mm_scaler = preprocessing.MinMaxScaler()
x_train = mm_scaler.fit_transform(x_train)
x_test = mm_scaler.transform(x_test)

x_train_df = pd.DataFrame(x_train)
x_test_df = pd.DataFrame(x_test)

#x_train = x_train.reset_index(drop=True)
#x_train_temp = x_train_temp.reset_index(drop=True)
#x_train=x_train.join(x_train_temp)

#x_train.head(5)

#x_test = x_test.reset_index(drop=True)
#x_test_temp = x_test_temp.reset_index(drop=True)
#x_test=x_test.join(x_test_temp)

#x_test.head(5)

#x_train = x_train.drop(cols,axis=1)
#x_test = x_test.drop(cols,axis=1)

x_train.dtypes

x_train_df.to_csv(path_or_buf ='/content/drive/My Drive/Practicum Data Set/x_train.csv',sep=',', header=True)
x_test_df.to_csv(path_or_buf ='/content/drive/My Drive/Practicum Data Set/x_test.csv',sep=',', header=True)
y_train.to_csv(path_or_buf ='/content/drive/My Drive/Practicum Data Set/y_train.csv',sep=',', header=True)
y_test.to_csv(path_or_buf ='/content/drive/My Drive/Practicum Data Set/y_test.csv',sep=',', header=True)

corr = x_train_df.corr()
corr.style.background_gradient(cmap='coolwarm')
corr.style.background_gradient(cmap='coolwarm').set_precision(2)

"""## Modelling"""

x_train = pd.read_csv('/content/drive/My Drive/Practicum Data Set/x_train.csv', index_col=0)

x_test = pd.read_csv('/content/drive/My Drive/Practicum Data Set/x_test.csv', index_col=0)

y_train = pd.read_csv('/content/drive/My Drive/Practicum Data Set/y_train.csv', index_col=0)
y_test = pd.read_csv('/content/drive/My Drive/Practicum Data Set/y_test.csv', index_col=0)

x_train.co

x_test.shape

pip install catboost

gc.collect()

"""# DecisionTreeClassifier"""

from sklearn.feature_selection import RFE
from sklearn.tree import DecisionTreeClassifier

model = DecisionTreeClassifier(random_state = 13, max_features = 'sqrt')

rfe = RFE(model, 10)

rfe = rfe.fit(x_train, y_train)
# print summaries for the selection of attributes
print(rfe.support_)
print(rfe.ranking_)

model.fit(x_train, y_train)

from sklearn import metrics 
y_pred=model.predict(x_test)

print("Accuracy:",metrics.accuracy_score(y_test, y_pred))

from sklearn.metrics import roc_auc_score

# Calculate roc auc
roc_value = roc_auc_score(y_test, y_pred)
roc_value

from sklearn.metrics import confusion_matrix, classification_report
print(classification_report(y_test, y_pred))
confusion_matrix(y_test, y_pred)

feature_imp = pd.Series(model.feature_importances_,index=x_train_df.columns).sort_values(ascending=False)
feature_imp



"""# LogisticRegression"""

from sklearn.linear_model import LogisticRegression

# all parameters not specified are set to their defaults
logisticRegr = LogisticRegression()
logisticRegr.fit(x_train, y_train)

rfe = RFE(logisticRegr, 8)

rfe = rfe.fit(x_train, y_train)
# print summaries for the selection of attributes
print(rfe.support_)
print(rfe.ranking_)

logisticRegr.fit(x_train, y_train)

from sklearn import metrics 
y_pred=logisticRegr.predict(x_test)

print("Accuracy:",metrics.accuracy_score(y_test, y_pred))

from sklearn.metrics import roc_auc_score

# Calculate roc auc
roc_value = roc_auc_score(y_test, y_pred)
roc_value

from sklearn.metrics import confusion_matrix, classification_report
print(classification_report(y_test, y_pred))
confusion_matrix(y_test, y_pred)

feature_imp = pd.Series(model.feature_importances_,index=x_train_df.columns).sort_values(ascending=False)
feature_imp



"""# Random Forest"""

from sklearn.ensemble import RandomForestClassifier

# all parameters not specified are set to their defaults
randomForest = RandomForestClassifier()
randomForest.fit(x_train, y_train)

rfe = RFE(randomForest, 8)

rfe = rfe.fit(x_train, y_train)
# print summaries for the selection of attributes
print(rfe.support_)
print(rfe.ranking_)

from sklearn import metrics 
y_pred=randomForest.predict(x_test)

print("Accuracy:",metrics.accuracy_score(y_test, y_pred))

from sklearn.metrics import roc_auc_score

# Calculate roc auc
roc_value = roc_auc_score(y_test, y_pred)
roc_value

from sklearn.metrics import confusion_matrix, classification_report
print(classification_report(y_test, y_pred))
confusion_matrix(y_test, y_pred)

feature_imp = pd.Series(model.feature_importances_,index=x_train_df.columns).sort_values(ascending=False)
feature_imp

"""#Random Forest Tuned"""

from sklearn.ensemble import RandomForestClassifier
randomForest = RandomForestClassifier(n_estimators=501, 
                               bootstrap = True,
                               max_features = 'sqrt',
                               random_state = 13)

randomForest.fit(x_train, y_train)

from sklearn import metrics 
y_pred=randomForest.predict(x_test)

print("Accuracy:",metrics.accuracy_score(y_test, y_pred))

from sklearn.metrics import roc_auc_score

# Calculate roc auc
roc_value = roc_auc_score(y_test, y_pred)
roc_value

from sklearn.metrics import confusion_matrix, classification_report
print(classification_report(y_test, y_pred))
confusion_matrix(y_test, y_pred)

feature_imp = pd.Series(model.feature_importances_,index=x_train_df.columns).sort_values(ascending=False)
feature_imp

"""# Bagging with Decision Tree Classifier"""

from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier

dtc = DecisionTreeClassifier(criterion="entropy")
bag_model=BaggingClassifier(base_estimator=dtc, n_estimators=100, bootstrap=True)
bag_model=bag_model.fit(x_train, y_train)



from sklearn import metrics 
y_pred=bag_model.predict(x_test)

print("Accuracy:",metrics.accuracy_score(y_test, y_pred))

from sklearn.metrics import roc_auc_score

# Calculate roc auc
roc_value = roc_auc_score(y_test, y_pred)
roc_value

from sklearn.metrics import confusion_matrix, classification_report
print(classification_report(y_test, y_pred))
confusion_matrix(y_test, y_pred)

feature_imp = pd.Series(model.feature_importances_,index=x_train_df.columns).sort_values(ascending=False)
feature_imp



"""# Gausian Naive Bayes"""

from sklearn.naive_bayes import GaussianNB
naiveBayes = GaussianNB()
naiveBayes = naiveBayes.fit(x_train, y_train)

from sklearn import metrics 
y_pred=naiveBayes.predict(x_test)

print("Accuracy:",metrics.accuracy_score(y_test, y_pred))

from sklearn.metrics import roc_auc_score

# Calculate roc auc
roc_value = roc_auc_score(y_test, y_pred)
roc_value

from sklearn.metrics import confusion_matrix, classification_report
print(classification_report(y_test, y_pred))
confusion_matrix(y_test, y_pred)

feature_imp = pd.Series(model.feature_importances_,index=x_train_df.columns).sort_values(ascending=False)
feature_imp

"""# ADA Boost"""

from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
ad = AdaBoostClassifier(DecisionTreeClassifier(),n_estimators=200)
ad.fit(x_train, y_train)

from sklearn import metrics 
y_pred=ad.predict(x_test)

print("Accuracy:",metrics.accuracy_score(y_test, y_pred))

from sklearn.metrics import roc_auc_score

# Calculate roc auc
roc_value = roc_auc_score(y_test, y_pred)
roc_value

from sklearn.metrics import confusion_matrix, classification_report
print(classification_report(y_test, y_pred))
confusion_matrix(y_test, y_pred)

feature_imp = pd.Series(ad.feature_importances_,index=x_train_df.columns).sort_values(ascending=False)
feature_imp